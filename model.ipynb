{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7107d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import numpy as np\n",
    "from  sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3363cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_data(data_path, with_dummies=True):\n",
    "    train_data = pd.read_csv(data_path)\n",
    "    train_data.drop(columns=[\"id\"], inplace=True)\n",
    "    train_data[[\"święto\", \"dzień_roboczy\"]] = train_data[[\"święto\", \"dzień_roboczy\"]].astype(int)\n",
    "    train_data[['year', 'month', 'day']] = train_data['data'].str.split('-', expand=True).astype(int)\n",
    "    train_data.drop(\"data\", axis=1, inplace=True)\n",
    "    train_data.drop(\"year\", axis=1, inplace=True)\n",
    "    train_data.drop(\"month\", axis=1, inplace=True)\n",
    "    train_data.drop(\"day\", axis=1, inplace=True)\n",
    "    if with_dummies:\n",
    "        return pd.get_dummies(train_data, columns=['pogoda'], dtype=int)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf1ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_validate_data():\n",
    "    train_data_with_dummies = preprocess_data(\"bit-x-adata/train.csv\")\n",
    "\n",
    "    X = train_data_with_dummies.drop(\"studenty_ms\", axis=1).values\n",
    "    Y = train_data_with_dummies[\"studenty_ms\"].values.reshape(-1,1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90fee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = get_train_and_validate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320d3c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20739034,  0.66269838, -1.37635436, ..., -0.3352392 ,\n",
       "        -0.50321961,  0.65947801],\n",
       "       [-0.20739034, -1.50898212, -1.92648776, ...,  2.98294472,\n",
       "        -0.50321961, -1.51635079],\n",
       "       [-0.20739034,  0.66269838, -1.04627432, ..., -0.3352392 ,\n",
       "         1.98720396, -1.51635079],\n",
       "       ...,\n",
       "       [-0.20739034,  0.66269838,  1.37431263, ..., -0.3352392 ,\n",
       "        -0.50321961,  0.65947801],\n",
       "       [-0.20739034, -1.50898212, -0.05603421, ..., -0.3352392 ,\n",
       "        -0.50321961,  0.65947801],\n",
       "       [-0.20739034,  0.66269838,  0.82417923, ..., -0.3352392 ,\n",
       "        -0.50321961,  0.65947801]], shape=(388, 9))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482e5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e1a86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6fe4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, pred, actual):\n",
    "        # ensure non-negative and stable log\n",
    "        pred_clamped = torch.clamp(pred, min=0.0)\n",
    "        actual_clamped = torch.clamp(actual, min=0.0)\n",
    "        # use log1p for stability\n",
    "        return torch.sqrt(self.mse(torch.log1p(pred_clamped + self.eps), torch.log1p(actual_clamped + self.eps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e07f35f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, y):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.y[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e37b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST KAGGLE\n",
    "# model loss = 0.87\n",
    "class GepardPred(nn.Module):\n",
    "    def __init__(self, input_size: int, dropout_rate: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8a6f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "learning_rate = 1e-3\n",
    "dropout_p = 0.3\n",
    "l2_reg = 1e-4\n",
    "batch_size = 128\n",
    "max_epochs = 300\n",
    "\n",
    "early_stopping_patience = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b307960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b386935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: GepardPred, X, y, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(X, dtype=torch.float32)\n",
    "        targets = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # outputs = model.predict_int(inputs)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "    return {\"loss\": loss.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b92bce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 2.7396, eval loss 2.8022\n",
      "Epoch 1 train loss: 2.3450, eval loss 2.4135\n",
      "Epoch 2 train loss: 1.7159, eval loss 1.8883\n",
      "Epoch 3 train loss: 1.0998, eval loss 1.4733\n",
      "Epoch 4 train loss: 0.7005, eval loss 1.2548\n",
      "Epoch 5 train loss: 0.3249, eval loss 1.1889\n",
      "Epoch 6 train loss: 0.3398, eval loss 1.1779\n",
      "Epoch 7 train loss: 0.3732, eval loss 1.1510\n",
      "Epoch 8 train loss: 0.4293, eval loss 1.0964\n",
      "Epoch 9 train loss: 0.2351, eval loss 1.0303\n",
      "Epoch 10 train loss: 0.2746, eval loss 0.9739\n",
      "Epoch 11 train loss: 0.2139, eval loss 0.9385\n",
      "Epoch 12 train loss: 0.2424, eval loss 0.9267\n",
      "Epoch 13 train loss: 0.1960, eval loss 0.9226\n",
      "Epoch 14 train loss: 0.1747, eval loss 0.9139\n",
      "Epoch 15 train loss: 0.2612, eval loss 0.8978\n",
      "Epoch 16 train loss: 0.2082, eval loss 0.8856\n",
      "Epoch 17 train loss: 0.2352, eval loss 0.8756\n",
      "Epoch 18 train loss: 0.1496, eval loss 0.8715\n",
      "Epoch 19 train loss: 0.2711, eval loss 0.8680\n",
      "Epoch 20 train loss: 0.1552, eval loss 0.8660\n",
      "Epoch 21 train loss: 0.2028, eval loss 0.8664\n",
      "Epoch 22 train loss: 0.1606, eval loss 0.8661\n",
      "Epoch 23 train loss: 0.1304, eval loss 0.8639\n",
      "Epoch 24 train loss: 0.1257, eval loss 0.8597\n",
      "Epoch 25 train loss: 0.0931, eval loss 0.8538\n",
      "Epoch 26 train loss: 0.1069, eval loss 0.8505\n",
      "Epoch 27 train loss: 0.2517, eval loss 0.8525\n",
      "Epoch 28 train loss: 0.2351, eval loss 0.8600\n",
      "Epoch 29 train loss: 0.0714, eval loss 0.8667\n",
      "Epoch 30 train loss: 0.1833, eval loss 0.8637\n",
      "Epoch 31 train loss: 0.0849, eval loss 0.8556\n",
      "Epoch 32 train loss: 0.2504, eval loss 0.8548\n",
      "Epoch 33 train loss: 0.1522, eval loss 0.8558\n",
      "Epoch 34 train loss: 0.1768, eval loss 0.8570\n",
      "Epoch 35 train loss: 0.1128, eval loss 0.8612\n",
      "Epoch 36 train loss: 0.1202, eval loss 0.8631\n",
      "Epoch 37 train loss: 0.1015, eval loss 0.8591\n",
      "Epoch 38 train loss: 0.1772, eval loss 0.8560\n",
      "Epoch 39 train loss: 0.1584, eval loss 0.8550\n",
      "Epoch 40 train loss: 0.1632, eval loss 0.8563\n",
      "Epoch 41 train loss: 0.3084, eval loss 0.8571\n",
      "Epoch 42 train loss: 0.1264, eval loss 0.8569\n",
      "Epoch 43 train loss: 0.1078, eval loss 0.8584\n",
      "Epoch 44 train loss: 0.1238, eval loss 0.8590\n",
      "Epoch 45 train loss: 0.0942, eval loss 0.8615\n",
      "Epoch 46 train loss: 0.0802, eval loss 0.8607\n",
      "Epoch 47 train loss: 0.1445, eval loss 0.8575\n",
      "Epoch 48 train loss: 0.1259, eval loss 0.8589\n",
      "Epoch 49 train loss: 0.2708, eval loss 0.8611\n",
      "Epoch 50 train loss: 0.0863, eval loss 0.8641\n",
      "Epoch 51 train loss: 0.1197, eval loss 0.8629\n",
      "Epoch 52 train loss: 0.1714, eval loss 0.8570\n",
      "Epoch 53 train loss: 0.2556, eval loss 0.8574\n",
      "Epoch 54 train loss: 0.1543, eval loss 0.8586\n",
      "Epoch 55 train loss: 0.1021, eval loss 0.8630\n",
      "Epoch 56 train loss: 0.0990, eval loss 0.8737\n",
      "Epoch 57 train loss: 0.0666, eval loss 0.8752\n",
      "Epoch 58 train loss: 0.1088, eval loss 0.8653\n",
      "Epoch 59 train loss: 0.0826, eval loss 0.8551\n",
      "Epoch 60 train loss: 0.1592, eval loss 0.8542\n",
      "Epoch 61 train loss: 0.1503, eval loss 0.8561\n",
      "Epoch 62 train loss: 0.0597, eval loss 0.8630\n",
      "Epoch 63 train loss: 0.1055, eval loss 0.8613\n",
      "Epoch 64 train loss: 0.0777, eval loss 0.8605\n",
      "Epoch 65 train loss: 0.0911, eval loss 0.8639\n",
      "Epoch 66 train loss: 0.1291, eval loss 0.8669\n",
      "Epoch 67 train loss: 0.2480, eval loss 0.8699\n",
      "Epoch 68 train loss: 0.0496, eval loss 0.8733\n",
      "Epoch 69 train loss: 0.2327, eval loss 0.8713\n",
      "Epoch 70 train loss: 0.1920, eval loss 0.8633\n",
      "Epoch 71 train loss: 0.0872, eval loss 0.8613\n",
      "Epoch 72 train loss: 0.1782, eval loss 0.8614\n",
      "Epoch 73 train loss: 0.0988, eval loss 0.8649\n",
      "Epoch 74 train loss: 0.1018, eval loss 0.8723\n",
      "Epoch 75 train loss: 0.0519, eval loss 0.8748\n",
      "Epoch 76 train loss: 0.1718, eval loss 0.8610\n",
      "Epoch 77 train loss: 0.2154, eval loss 0.8580\n",
      "Epoch 78 train loss: 0.1836, eval loss 0.8618\n",
      "Epoch 79 train loss: 0.1017, eval loss 0.8600\n",
      "Epoch 80 train loss: 0.1603, eval loss 0.8665\n",
      "Epoch 81 train loss: 0.2000, eval loss 0.8716\n",
      "Epoch 82 train loss: 0.1831, eval loss 0.8675\n",
      "Epoch 83 train loss: 0.1332, eval loss 0.8646\n",
      "Epoch 84 train loss: 0.2025, eval loss 0.8634\n",
      "Epoch 85 train loss: 0.1009, eval loss 0.8636\n",
      "Epoch 86 train loss: 0.1666, eval loss 0.8649\n",
      "Epoch 87 train loss: 0.0428, eval loss 0.8643\n",
      "Epoch 88 train loss: 0.0886, eval loss 0.8641\n",
      "Epoch 89 train loss: 0.1321, eval loss 0.8652\n",
      "Epoch 90 train loss: 0.0865, eval loss 0.8675\n",
      "Epoch 91 train loss: 0.0877, eval loss 0.8686\n",
      "Epoch 92 train loss: 0.1746, eval loss 0.8694\n",
      "Epoch 93 train loss: 0.2953, eval loss 0.8709\n",
      "Epoch 94 train loss: 0.2116, eval loss 0.8688\n",
      "Epoch 95 train loss: 0.1860, eval loss 0.8671\n",
      "Epoch 96 train loss: 0.2634, eval loss 0.8656\n",
      "Epoch 97 train loss: 0.0799, eval loss 0.8649\n",
      "Epoch 98 train loss: 0.1273, eval loss 0.8646\n",
      "Epoch 99 train loss: 0.1184, eval loss 0.8672\n",
      "Epoch 100 train loss: 0.2237, eval loss 0.8664\n",
      "Epoch 101 train loss: 0.0851, eval loss 0.8650\n",
      "Epoch 102 train loss: 0.1711, eval loss 0.8688\n",
      "Epoch 103 train loss: 0.1540, eval loss 0.8696\n",
      "Epoch 104 train loss: 0.1408, eval loss 0.8746\n",
      "Epoch 105 train loss: 0.2466, eval loss 0.8741\n",
      "Epoch 106 train loss: 0.1444, eval loss 0.8700\n",
      "Epoch 107 train loss: 0.1222, eval loss 0.8683\n",
      "Epoch 108 train loss: 0.1199, eval loss 0.8685\n",
      "Epoch 109 train loss: 0.1130, eval loss 0.8690\n",
      "Epoch 110 train loss: 0.1235, eval loss 0.8716\n",
      "Epoch 111 train loss: 0.1473, eval loss 0.8739\n",
      "Epoch 112 train loss: 0.1844, eval loss 0.8789\n",
      "Epoch 113 train loss: 0.2248, eval loss 0.8801\n",
      "Epoch 114 train loss: 0.1607, eval loss 0.8753\n",
      "Epoch 115 train loss: 0.1950, eval loss 0.8710\n",
      "Epoch 116 train loss: 0.0580, eval loss 0.8703\n",
      "Epoch 117 train loss: 0.1122, eval loss 0.8726\n",
      "Epoch 118 train loss: 0.1441, eval loss 0.8740\n",
      "Epoch 119 train loss: 0.1998, eval loss 0.8792\n",
      "Epoch 120 train loss: 0.2426, eval loss 0.8828\n",
      "Epoch 121 train loss: 0.1583, eval loss 0.8831\n",
      "Epoch 122 train loss: 0.1310, eval loss 0.8836\n",
      "Epoch 123 train loss: 0.1920, eval loss 0.8818\n",
      "Epoch 124 train loss: 0.2125, eval loss 0.8754\n",
      "Epoch 125 train loss: 0.1689, eval loss 0.8755\n",
      "Epoch 126 train loss: 0.1844, eval loss 0.8741\n",
      "Early stopping at epoch 126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GepardPred(\n",
    "    input_size=X_train.shape[1],\n",
    "    dropout_rate=dropout_p\n",
    ")\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=l2_reg\n",
    ")\n",
    "\n",
    "train_dataset = MyDataset(\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "loss_fn = RMSLELoss()\n",
    "\n",
    "best_model = model\n",
    "best_threshold = None\n",
    "patience = 100\n",
    "steps_without_improvement = 0\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    valid_metrics = evaluate_model(model, X_valid, y_valid, loss_fn)\n",
    "\n",
    "    valid_loss = valid_metrics[\"loss\"]\n",
    "\n",
    "    print(f\"Epoch {epoch} train loss: {loss.item():.4f}, eval loss {valid_loss:.4f}\")\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        best_model = deepcopy(model)\n",
    "\n",
    "        steps_without_improvement = 0\n",
    "    else:\n",
    "        steps_without_improvement += 1\n",
    "        if steps_without_improvement >= patience:\n",
    "\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef3f08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = preprocess_data(\"bit-x-adata/test.csv\").values\n",
    "scaler = StandardScaler()\n",
    "X_test = scaler.fit_transform(final_test_data)\n",
    "X_test = torch.from_numpy(X_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5465b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_csv(model, X_test, path=\"bit-x-adata/test.csv\"):\n",
    "    model.eval()\n",
    "    test_data = pd.read_csv(path)\n",
    "    ids = test_data[\"id\"].values\n",
    "\n",
    "    y_pred = model(X_test)\n",
    "    y_pred = y_pred.detach().numpy().flatten()\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": ids,\n",
    "        \"studenty_ms\": y_pred.astype(np.int64)\n",
    "    })\n",
    "\n",
    "    submission.to_csv('comunicadores_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad3c587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_to_csv(best_model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca1a2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF valid RMSLE: 1.4680854099876028\n",
      "NN valid RMSLE: 0.8504811425012122\n",
      "Ensemble valid RMSLE: 1.21458686333347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def rmsle_np(y_true, y_pred):\n",
    "    y_t = np.clip(y_true, 0, None)\n",
    "    y_p = np.clip(y_pred, 0, None)\n",
    "    return np.sqrt(np.mean((np.log1p(y_p) - np.log1p(y_t)) ** 2))\n",
    "\n",
    "# get fresh numpy train/valid (same preprocessing as used for NN)\n",
    "X_train_np, X_valid_np, y_train_np, y_valid_np = get_train_and_validate_data()\n",
    "# ensure 1D targets for sklearn\n",
    "y_train_1d = y_train_np.ravel()\n",
    "y_valid_1d = y_valid_np.ravel()\n",
    "\n",
    "# train RF\n",
    "rf = RandomForestRegressor(n_estimators=300, max_depth=12, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train_np, y_train_1d)\n",
    "\n",
    "# evaluate RF\n",
    "rf_pred_valid = rf.predict(X_valid_np)\n",
    "print(\"RF valid RMSLE:\", rmsle_np(y_valid_1d, rf_pred_valid))\n",
    "\n",
    "# evaluate NN (best_model) on same validation set\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_inputs = torch.tensor(X_valid_np, dtype=torch.float32)\n",
    "    nn_out = best_model(nn_inputs).cpu().numpy().flatten()\n",
    "print(\"NN valid RMSLE:\", rmsle_np(y_valid_1d, nn_out))\n",
    "\n",
    "# simple ensemble (average)\n",
    "ensemble_pred = (rf_pred_valid + nn_out) / 2.0\n",
    "print(\"Ensemble valid RMSLE:\", rmsle_np(y_valid_1d, ensemble_pred))\n",
    "\n",
    "# helper to produce submission using RF or ensemble\n",
    "def pred_to_csv_with_rf(rf_model, nn_model, X_test, use_ensemble=False, path=\"bit-x-adata/test.csv\"):\n",
    "    test_df = pd.read_csv(path)\n",
    "    ids = test_df[\"id\"].values\n",
    "\n",
    "    rf_preds = rf_model.predict(X_test)\n",
    "    if use_ensemble:\n",
    "        nn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            nn_preds = nn_model(torch.from_numpy(X_test).float()).cpu().numpy().flatten()\n",
    "        preds = (rf_preds + nn_preds) / 2.0\n",
    "    else:\n",
    "        preds = rf_preds\n",
    "\n",
    "    preds = np.round(np.clip(preds, 0, None)).astype(np.int64)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": ids,\n",
    "        \"studenty_ms\": preds\n",
    "    })\n",
    "    submission.to_csv(\"comunicadores_submission_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e11ea6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_csv_rf(model, X_test, path=\"bit-x-adata/test.csv\"):\n",
    "    test_data = pd.read_csv(path)\n",
    "    ids = test_data[\"id\"].values\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": ids,\n",
    "        \"studenty_ms\": y_pred.astype(np.int64)\n",
    "    })\n",
    "\n",
    "    submission.to_csv('comunicadores_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e78697c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_to_csv_rf(rf, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackhathons",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
